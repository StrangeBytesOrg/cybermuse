name: App Build

on:
  workflow_dispatch:
  push:
    branches:
      - ci

env:
  # Necessary for most environments as build failure can occur due to OOM issues
  NODE_OPTIONS: "--max-old-space-size=4096"
  LLAMA_CPP_VERSION: "b2961"
  APP_VERSION: "v0.${{ github.run_number }}"

jobs:
  build:
    env:
      LLAMA_CPP_VERSION: "b2961"
    strategy:
      fail-fast: false
      matrix:
        build:
          - name: chat
            platform: linux/amd64
            os: ubuntu-latest
          - name: chat
            platform: windows/amd64
            os: windows-latest
          - name: chat # For Mac builds, the name MUST match the output name in the wails.json file
            platform: darwin/amd64
            os: macos-latest

    runs-on: ${{ matrix.build.os }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Get llama.cpp server binary (Linux)
        if: matrix.build.platform == 'linux/amd64'
        run: |
          curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-ubuntu-x64.zip > llamacpp.zip
          mkdir -p build/llamacpp
          unzip -j llamacpp.zip build/bin/server -d build/llamacpp/

      - name: Get llama.cpp server binary (Mac)
        if: matrix.build.platform == 'darwin/amd64'
        run: |
          curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-macos-x64.zip > llamacpp.zip
          mkdir -p build/llamacpp
          unzip -j llamacpp.zip build/bin/server -d build/llamacpp/

      - name: Get llama.cpp server binary (Windows)
        if: matrix.build.platform == 'windows/amd64'
        run: |
          curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-win-avx-x64.zip > llamacpp.zip
          mkdir -p build/llamacpp
          unzip -j llamacpp.zip server.exe -d build/llamacpp/
          unzip -j llamacpp.zip llama.dll -d build/llamacpp/
          
      - name: Build Wails
        uses: dAppServer/wails-build-action@v2.2
        id: build
        with:
          build-name: ${{ matrix.build.name }}
          build-platform: ${{ matrix.build.platform }}
          package: false # Upload artifacts
          go-version: '1.22.2'

      - name: Archive Linux
        if: matrix.build.platform == 'linux/amd64'
        run: cd build/bin && tar -czf chat-linux-${{env.APP_VERSION}}.tar.gz chat

      - name: Archive Windows
        if: matrix.build.platform == 'windows/amd64'
        run: |
          cd build/bin
          7z a -tzip chat-windows-${{env.APP_VERSION}}.zip chat-amd64-installer.exe

      - name: Archive Mac
        if: matrix.build.platform == 'darwin/amd64'
        run: cd build/bin && mv chat.app.zip chat-mac-${{env.APP_VERSION}}.zip

      - name: Setup RClone
        uses: AnimMouse/setup-rclone@v1
        with:
          disable_base64: true
          rclone_config: |
            [r2]
            type = s3
            provider = Cloudflare
            access_key_id = ${{ secrets.R2_ACCESS_KEY_ID }}
            secret_access_key = ${{ secrets.R2_SECRET_ACCESS_KEY }}
            endpoint = https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
            acl = private
            no_check_bucket = true

      - name: Upload files to R2
        run: 'rclone copy --include "*.zip" --include "*.tar.gz" build/bin/ r2:chat-app-releases/'
