name: App Build

on:
  workflow_dispatch:
  push:
    branches:
      - ci

env:
  # Necessary for most environments as build failure can occur due to OOM issues
  NODE_OPTIONS: "--max-old-space-size=4096"
  LLAMA_CPP_VERSION: "b3265"
  APP_VERSION: "v0.${{ github.run_number }}"
  VITE_CLIENT_VERSION: "v0.${{ github.run_number }}"

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        build:
          - name: build-linux
            os: ubuntu-latest
            platform: linux/amd64
          - name: build-mac
            os: macos-latest
            platform: darwin/amd64
          - name: build-windows
            os: windows-latest
            platform: windows/amd64

    runs-on: ${{ matrix.build.os }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20.14.0'

      - name: Setup RClone
        uses: AnimMouse/setup-rclone@v1
        with:
          disable_base64: true
          rclone_config: |
            [r2]
            type = s3
            provider = Cloudflare
            access_key_id = ${{ secrets.R2_ACCESS_KEY_ID }}
            secret_access_key = ${{ secrets.R2_SECRET_ACCESS_KEY }}
            endpoint = https://${{ secrets.R2_ACCOUNT_ID }}.r2.cloudflarestorage.com
            acl = private
            no_check_bucket = true

      # Get llama.cpp binaries specific to the platform
      - name: Get llama.cpp server binary
        if: matrix.build.platform == 'linux/amd64'
        run: curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-ubuntu-x64.zip > llamacpp.zip

      - name: Get llama.cpp server binary
        if: matrix.build.platform == 'darwin/amd64'
        run: curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-macos-x64.zip > llamacpp.zip

      - name: Get llama.cpp server binary
        if: matrix.build.platform == 'windows/amd64'
        run: curl --silent -L https://github.com/ggerganov/llama.cpp/releases/download/${{env.LLAMA_CPP_VERSION}}/llama-${{env.LLAMA_CPP_VERSION}}-bin-win-vulkan-x64.zip > llamacpp.zip

      - name: Extract
        run: unzip -j llamacpp.zip -d llamacpp/

      - name: Install Node Dependencies
        run: npm ci

      - name: Build Frontend
        run: npm run build

      - name: Build Electron
        run: npm run make

      - name: Upload Linux
        if: matrix.build.target-platform == 'linux/amd64'
        run: |
          cd out
          rclone copy chat-linux-x64.tar.xz r2:chat-app-releases/latest/
          rclone copyto chat-linux-x64.tar.xz r2:chat-app-releases/versions/chat-${{env.APP_VERSION}}-linux.tar.gz
        
      - name: Upload Mac
        if: matrix.build.target-platform == 'darwin/amd64'
        run: |
          cd out
          rclone copy chat-mac-x64.zip r2:chat-app-releases/latest/
          rclone copyto chat-mac-x64.zip r2:chat-app-releases/versions/chat-${{env.APP_VERSION}}-mac-x64.zip
          rclone copy chat-mac-arm64.zip r2:chat-app-releases/latest/
          rclone copyto chat-mac-arm64.zip r2:chat-app-releases/versions/chat-${{env.APP_VERSION}}-mac-arm64.zip

      - name: Archive and Upload Windows
        if: matrix.build.target-platform == 'windows/amd64'
        run: |
          cd out
          Compress-Archive -Path chat-win-x64.zip -DestinationPath chat-win-x64.zip
          rclone copy chat-win-x64.zip r2:chat-app-releases/latest/
          rclone copyto chat-win-x64.zip r2:chat-app-releases/versions/chat-${{env.APP_VERSION}}-win-x64.zip
